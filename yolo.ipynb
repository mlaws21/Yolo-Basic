{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementing YOLO v1 in PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <center><img src=\"./img/predictions.jpg\" width=\"750\"/> </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. What is YOLO?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <center><img src=\"./img/paper.png\" width=\"400\"/> </center>\n",
    "</div>\n",
    "\n",
    "Developed in 2015 by Redmon et. al., YOLO (You Only Look Once) is a deep learning architecture which greatly improved the efficiency of image detection networks. The goal is simply: to detect objects appearing in an image, with their corresponding bounding boxes (see above image). Previous networks would train separately for category detection (dog, bicyle, laptop, e.g.) and bounding box detection (where an object appears in an image), going back and forth between the two to refine predictions. Redmon et. al. proposed a novel method in which bounding boxes and categories were predicted together in one pass through a single model, hence the model's name *You Only Look Once.*\n",
    "\n",
    "#### **Why is it an important problem?**\n",
    "\n",
    "Image recognition is an important problem with a growing field of different approches including, most recently, neural networks. Image recognition has a wide variety of uses, some of which include: [security](https://ieeexplore.ieee.org/abstract/document/8376292), [self-driving cars](https://ieeexplore.ieee.org/abstract/document/8595590), [cancer regonition](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10377683/), and [zebra detection](https://markandrewhopkins.com/csci-381-deep-learning/). Developing fast image recognition and classification techniques is an important goal in deep learning, and it is a quickly growing industry, [expected to reach a $22.64 billion market volume by 2030](https://www.statista.com/outlook/tmo/artificial-intelligence/computer-vision/image-recognition/worldwide).\n",
    "\n",
    "#### **Main contributions of YOLO**\n",
    "\n",
    "YOLO's main contribution to the study of object recognition is its speed while maintaining correctness. As touched on above by calculating both the bounding boxes and the categorization in a single pass, YOLO completely outperformed the then current the state-of-the-art (Fast R-CNN). YOLO's original code is implemented entirely in C which also contributed to the immense speed up. Additionally compared to Fast R-CNN, YOLO made fewer background mistakes and was able to augment Fast R-CNN to produce an improved mAP score. Overall YOLO was the first object detection software that could run in real time with mostly correct predictions. Running at 45 frames per second with a corrisponding mAP score of 63.4, YOLO exceeds the mAP of other real time detectors by over 2 times; and compared to non-real-time detects only loses ~10 mAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. YOLO v1 at a High Level**\n"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "YOLO, as its name suggests, predicts image categories and bounding boxes in *one pass,* greatly improving the efficiency of image classification tasks. Its architecture is relatively simple, using several convolutional layers of varying sizes, a custom loss function, and leaky ReLU. The following section describes the sources of YOLO training data, data import format, architecture, loss, and evaluation of YOLO v1.\n",
     "\n",
     "#### **Data and Importing**\n",
     "\n",
     "YOLO v1 pretrains its convolutional layers on the [1000-class ImageNet competition dataset](https://www.image-net.org/index.php). This training only predicts categories for full images, not bounding boxes, so the model is then trained on the [PASCAL Visual Object Classes (VOC) 2007 dataset](http://host.robots.ox.ac.uk/pascal/VOC/), which includes both category and bounding box data.\n",
     "\n",
     "Images from both ImageNet and PASCAL VOC are first resized to 448 x 448 then fed through the CNN. The VOC training response data is formatted into $N \\times S \\times S \\times 25$ tensors, where $N$ is the number of batches, $S$ is the grid size applied to the image, and the final dimension of length 25 takes this format:\n",
     "\n",
     "<div>\n",
     "     <center><img src=\"./img/in-vector.png\" width=\"750\"/> </center>\n",
     "</div>\n",
     "\n",
     "On the prediction side, we expect a slightly modified version of the above format: in this case, each cell of the $S \\times S$ grid predicts a 30-length tensor, where the first 20 entries are probability predictions instead of a one-hot encoding, and the tensor predicts 2 sets of bounding boxes instead of one true box:\n",
     "\n",
     "<div>\n",
     "     <center><img src=\"./img/out-vector.png\" width=\"800\"/> </center>\n",
     "</div>\n",
     "\n",
     "#### **Architecture**\n",
     "\n",
     "<div>\n",
     "     <center><img src=\"./img/arch.png\" width=\"1080\"/> </center>\n",
     "</div>\n",
     "\n",
     "YOLO's architecture consists of 24 convolutional layers followed by 2 fully connected layers detailed above. The 1x1 convolutional layers are in place to reduce the feature space from preceding layers. This may be confusing at first as a 1x1 convolution with a stride of 1 just copies the image however; roughly speaking, different features of the input can be \"learned\" by different kernels of the 1x1 layer thus reducing the feature space. \n",
     "\n",
     "Object detection is a hard task, and at this point had not been achieved in real time. In order to simplify the task YOLO was first pre-trained to recognize images. For pre-training only the first 20 layers were trained followed by an average-pooling and fully connected layer --- these additional layer were truncated in the actual YOLO model.Using the ImageNet 2012 data the model was trained to recognize images from one of 20 catagories. After a week of training, the model achieved a top-5 accuracy of 88\\% (meaning each of the network contained the correct answer in the top 5 catagories it predicted 88\\% of the time).\n",
     "\n",
     "The intuitive reasoning for YOLO's pre-training can be explained with a simple example. Say I gave you an image and asked you to find every fish in the image and draw a box around it (this is essentially the YOLO task). You could probably do it --- but say you didn't know what a fish actually looked like, the task becomes significantly harder. The same is true with YOLO, by teaching it how to recognize images in a well defined dataset of ImageNet it will help make predicting bounding boxes possible.\n",
     "\n",
     "Up until the last layer, YOLO is just a standard CNN. In each layer of the CNN, it learns something about the pixel and the surrounding pixels in order to make its final prediction. To prevent overfitting a dropout layer with p = 0.5 is added after the first connected layer. Additionally YOLO randomly augments the data by changing the size, exposure and saturation to help with overfitting. To enforce non-linearity the leaky rectified linear unit (leaky ReLU) activation function is used with the parameter 0.1 following each layer besides the final layer. The final layer is special and uses a linear activation.\n",
     "\n",
     "The final prediction of YOLO is where the novelty of the model comes from. The last dense layer is reshaped into a SxSxC+(B*5) tensor where S is the chosen grid size, C is the number of catagories and B is number of boxes predicted in each grid square. YOLO uses a grid size of 7x7 and 2 boxes per square to predict across 20 catagories resulting in an output of size 7x7x30. An example is seen in the image below. This approach allows both the catagory and the bounding boxes to be predicted simultaniously for fast unified detection. Working with this output is less straightforward then something like a series of probability weights or a single value; however YOLO provides a unique loss function to handle this. \n",
     "\n",
     "\n",
     "\n",
     "<div>\n",
     "     <center><img src=\"./img/grid.png\" width=\"750\"/> </center>\n",
     "</div>\n",
     "\n",
     "#### **Loss**\n",
     "\n",
     "YOLO v1 implements a custom loss function, described in their paper in the following equation:\n",
     "\n",
     "<div>\n",
     "     <center><img src=\"./img/loss.png\" width=\"900\"/> </center>\n",
     "</div>\n",
     "\n",
     "This may look overwhelming at first, but it's not overly complicated. The loss function takes the SSE across the predicted $(x,y)$ box midpoint, $(h,w)$ box dimensions, the predicted probability of an object in the box, and the set of $c_i$ category probabilities. The first and second line compute SSE loss for the midpoint coordinates and dimensions, the next two lines compute the SSE loss for object confidence (whether an object appears in the box), and the last line computes the SSE for category probability predictions.\n",
     "\n",
     "**Some important symbols to note:**\n",
     "\n",
     "&nbsp;&nbsp;&nbsp;&nbsp; $ùüô_{i}^{\\textrm{obj}}$ indicates whether an object appears in cell $i$\n",
     "\n",
     "&nbsp;&nbsp;&nbsp;&nbsp; $ùüô_{ij}^{\\textrm{obj}}$ indicates whether the $j^{\\textrm{th}}$ bounding box predictor in cell $i$ is \"responsible\" for that prediction (highest intersection over union, or IOU)\n",
     "\n",
     "&nbsp;&nbsp;&nbsp;&nbsp; $C_i$ indicates whether box $i$ contains an object in the training data\n",
     "\n",
     "&nbsp;&nbsp;&nbsp;&nbsp; $\\hat{C}_i$ is the predicted probability that an object appears in box $i$\n",
     "\n",
     "&nbsp;&nbsp;&nbsp;&nbsp; $\\lambda_{\\textrm{coord}}$ is a learning parameter set to 5\n",
     "\n",
     "&nbsp;&nbsp;&nbsp;&nbsp; $\\lambda_{\\textrm{noobj}}$ is a learning parameter set to 0.5\n",
     "\n",
     "#### **Evaluation**\n",
     "\n",
     "There are two main metrics that YOLO evaluates itself on: speed and mean average precision (mAP). Since YOLO is a real time detection tool its speed in measured by how many image frames can it predict per second (fps). This metric is important because this is the fps it can accurately translate video at. YOLO is able to translate at 45fps, significantly faster than any detector with similar mAP scores. For reference the human eye on average can only see between 30fps and 60fps, thus 45fps is plenty fast for real time translation. Fast YOLO, a smaller version translates at 155fps for a moderate hit to mAP.\n",
     "\n",
     "The second evaluation metric is mAP. Essentially, mAP is a measure of correctness of the prediction. Generally speaking mAP refers to the area under the precision-recall curve. To understand this first we need to understand precision and recall. Precision measures of all positive predicts the model made how many were correct, i.e. $\\frac{\\text{correct positive predictions}}{\\text{all positive predictions}}$ where in our case a \"positive prediction\" refers to predicting an object. Recall measures of all predictions how many were positive and correct i.e. $\\frac{\\text{correct positive predictions}}{\\text{all predictions}}$. Each of these metrics on there own are not particularly useful as predicting positive for all instances gets a perfect recall and predicting a small subset of easy to predict images gets perfect precision. However, if we can encapsulate both metrics into one then we can optimize it. We do this using the precison-recall curve which plots precison as a function of recall. To evaluate then all we have to do is take the integral of the precison-recall curve. \n",
     "\n",
     "For object detection the input to the recall and loss functions is whether or not the box is correct. To determine that we measure the Intersection Over Union (IOU) between the predicted boxes and the ground truth boxes in the image. If the IOU is over some threshold we declare that the box is correct otherwise we declare that it is incorrect. The threshold in this case is dependent on how \"good\" we want the boxes to be. The YOLO paper doesn't say what threshold they use, but other implementations use 80\\%.\n",
     "\n",
     "For its speed YOLO significantly outperforms the state-of-the-art mAP scores only losing to Fast R-CNN, and Faster R-CNN VGG-16 that run at 0.5 and 7fps respectively. \n",
     "\n",
     "Below we provide a table that compares the mAP and fps of different object detectors.  \n",
     "\n",
     "<div>\n",
     "     <center><img src=\"./img/eval.png\" width=\"400\"/> </center>\n",
     "</div>"
    ]
   },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Evaluation**\n",
    "\n",
    "There are two main metrics that YOLO evaluates itself on: speed and mean average precision (mAP). Since YOLO is a real time detection tool its speed in measured by how many image frames can it predict per second (fps). This metric is important because this is the fps it can accurately translate video at. YOLO is able to translate at 45fps, significantly faster than any detector with similar mAP scores. For reference the human eye on average can only see between 30fps and 60fps, thus 45fps is plenty fast for real time translation. Fast YOLO, a smaller version translates at 155fps for a moderate hit to mAP.\n",
    "\n",
    "The second evaluation metric is mAP. Essentially, mAP is a measure of correctness of the prediction. Generally speaking mAP refers to the area under the precision-recall curve. To understand this first we need to understand precision and recall. Precision measures of all positive predicts the model made how many were correct, i.e.\n",
    "\n",
    "$$\\frac{\\text{correct positive predictions}}{\\text{all positive predictions}}$$\n",
    "\n",
    "where in our case a \"positive prediction\" refers to predicting an object. Recall measures of all predictions how many were positive and correct i.e.\n",
    "\n",
    "$$\\frac{\\text{correct positive predictions}}{\\text{all predictions}}$$\n",
    "\n",
    "Each of these metrics on there own are not particularly useful as predicting positive for all instances gets a perfect recall and predicting a small subset of easy to predict images gets perfect precision. However, if we can encapsulate both metrics into one then we can optimize it. We do this using the precison-recall curve which plots precison as a function of recall. To evaluate then all we have to do is take the integral of the precison-recall curve. \n",
    "\n",
    "For object detection the input to the recall and loss functions is whether or not the box is correct. To determine that we measure the Intersection Over Union (IOU) between the predicted boxes and the ground truth boxes in the image. If the IOU is over some threshold we declare that the box is correct otherwise we declare that it is incorrect. The threshold in this case is dependent on how \"good\" we want the boxes to be. The YOLO paper doesn't say what threshold they use, but other implementations use 80\\%.\n",
    "\n",
    "For its speed YOLO significantly outperforms the state-of-the-art mAP scores only losing to Fast R-CNN, and Faster R-CNN VGG-16 that run at 0.5 and 7fps respectively. \n",
    "\n",
    "Below we provide a table that compares the mAP and fps of different object detectors.  \n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/eval.png\" width=\"400\"/> </center>\n",
    "</div>\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Our Simplified Implementation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a very basic version of YOLO called *YOLO-Basic* that can perform a simpler task than the original YOLO due to computing and time constraints. \n",
    "\n",
    "In order to speed up the YOLO training pipeline we made three significant changes. Firstly, we used synthetically genereated images of 5 different shapes (X, Circle, Square, Rectangle, Star. This helped remove background noise as shapes are generated on a plain white background allowing for easier recognition. It also reduced the dimensionality of the output because there were less catagories to predict. Secondly we reduce the size of the final output grid from 7x7 to 3x3 to greatly reduce the amount of predictions the model makes each minibatch. To that end we also have each grid cell only predict a single bounding box. These two changes combined greatly simplify our output as we have gone from a 7x7x(20 + 2\\*5) = 1470 parameter output shape to a 3x3x(5 + 1\\*5) = 90 parameter output shape. This reduction of dimentionality allows for out last significant change which was simplifying the network. We removed several convolutional layers and greatly reduced the kernel size at each layer to 32, down from some kernel sizes over 1000. Lastly, we provide the images as 112x112 images to simplify the input; without background noise sharper images are not needed for identification.\n",
    "\n",
    "Below we explain the simplification we did in more detail and provide a easily runnable set of code cells to run *YOLO-Basic* from start to finish. These code cells mostly import functions from other python files that we wrote so if you are concerned with implementation details you should view the other .py files in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS \n",
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS\n",
    "\n",
    "DATA_DIR = \"DEMO\"\n",
    "TRAIN_SIZE = 100\n",
    "TEST_SIZE = 10\n",
    "PRETRAINED_PARAMS = \"models/demo_pretrain.pt\"\n",
    "YOLO_PARAMS = \"models/demo_yolo.pt\"\n",
    "CATAGORIES = ['O', 'Pent', 'Square', 'Star', 'X']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data and Generation**\n",
    "\n",
    "As noted in the introduction we used synthetically generated data to train and test our *YOLO-Basic* model. To generate the images we used the python PIL library. We wrote 5 different functions that each generated a shape of random size in a random position on the screen. Each shape had an associated bounding box that completely surrounded the shape plus a small adjustment factor. Shapes were generated to be of reasonable size (between 1/8 and 1/2 of the image) and such that the entire shape was contained in the frame of the image. Example images can be seen after generating the data by running the corrisponding cell. \n",
    "\n",
    "To simulate the training proceedure of YOLO, the pretraining set is a different set of images than the YOLO training set. The images are generated as detailed above with 1000 images of each shape for training and 100 images of each shape for validation. These images were no longer used after the pretraining was completed.\n",
    "\n",
    "To generate the YOLO training and testing data, 10,000 images of each shape were generated to use as a training set for a total of 50,000 training images; 100 images of each shape were generated as a validation set to monitor the accuracy. With each image that was created an entry was also created into the data.json file for a given data directory. Eeach entry contains the shape label, the path to the image file, the partition (train / test) and the bounding box. The data loader can then use this json file to read in the data for training and testing. A preview of this json can be seen by running the corrisponding cell below.\n",
    "\n",
    "Note: while multiple shape generation is possible given our functions we only generated images with a single shape for training and testing for simplicity and time sake; however in theory *YOLO-Basic* could still recognize mulitple shapes in a single image.  \n",
    "\n",
    "To generate and visualize the data run the cells below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Data Directory DEMO already exists\n",
      "if you would like to continue run rm -r DEMO in your terminal\n"
     ]
    }
   ],
   "source": [
    "# DATA GENERATION\n",
    "from generate_data import generate_data\n",
    "\n",
    "generate_data(DATA_DIR, TRAIN_SIZE, TEST_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100%;\"><img src=\"DEMO/train/Square/28.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/O/28.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/Star/28.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/Pent/28.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/X/28.png\" style=\"width: 18%; margin: 10px; float: left;\" /></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DISPLAY GENERATED IMAGES\n",
    "\n",
    "num = random.randint(0, TRAIN_SIZE)\n",
    "image_paths = [os.path.join(DATA_DIR, \"train\", x, str(num) + \".png\") for x in os.listdir(os.path.join(DATA_DIR, \"train\"))]\n",
    "\n",
    "images_html = ''.join(f'<img src=\"{path}\" style=\"width: 18%; margin: 10px; float: left;\" />' for path in image_paths)\n",
    "\n",
    "html_content = f'<div style=\"width: 100%;\">{images_html}</div>'\n",
    "\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/0.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.58875,0.80875,0.1025,0.1025\"\n",
      "},\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/1.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.78875,0.34625,0.2775,0.2775\"\n",
      "},\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/2.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.7325,0.63,0.105,0.105\"\n",
      "},\n"
     ]
    }
   ],
   "source": [
    "### DISPLAY JSON DATA\n",
    "\n",
    "# json data stores ground truth data associated with the images.\n",
    "def display_file_head(file_path, num_lines=19):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for _ in range(num_lines):\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(line.strip())\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, \"data.json\")\n",
    "display_file_head(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loss**\n",
    "\n",
    "We modify the YOLO v1 loss to fit our simplified model. Namely, we reduce the number of category predictions from 20 (from the VOC dataset) to 5 (shapes), and we reduce from 2 bounding boxes to 1, and we reduce our grid size to 3. We define a custom loss function called `yolo_loss`, which takes in an $S$ value (grid size), a $B$ value (number of box predictions), and a $C$ value (number of categories). Corresponding to our modificatons, we set $S = 3$, $B = 1$, and $C = 5$.\n",
    "\n",
    "We borrow some code from [https://github.com/aladdinpersson](https://github.com/aladdinpersson) to implement the more tricky torch operations in calculating this loss. The full loss function, broken down into its constituent parts, can be found in `loss.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Architecture and Training**\n",
    "\n",
    "Overall, as mentioned in the intro, to fit our computational and time constraints, we greatly reduced the complexity of the models. We reduced the number of pretraining convolutional layers from 20 to 8 and the number of kernel all down to 32. We use the same amount of max pooling layer and layers with stride > 1 in order to reduce to the same grid size; however we add one extra maxpool layer at the end of the pretrain model to reduce what will become the grid size in yolo training to 3x3. \n",
    "\n",
    "For yolo training we add 4 new untrained layers as the paper suggests; however our total is sill 12 layers vs 24 layers. Furthermore, as in pretraining the number of kernels are all 32 down from 1024. \n",
    "\n",
    "For both pretraining and yolo training we use Leaky ReLU as our activation function with parameter 0.1 as described in the paper. We also implement a dropout layer with p = 0.5 between our fully connected layers as described by the paper. \n",
    "\n",
    "For training we used minibatch gradient decent with a batch size of 32 for both pretraining and yolo training. Pretraining was able to quickly converge to perfect accuracy on both the training and test set and stopped training early around 5 epochs. YOLO training ran for [TODO] epochs before it converged. We used the ADAM optimizer for our decent with learning rate of 0.001 and the default ADAM parameters.\n",
    "\n",
    "To build and train the model run the cells below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Layers:\n",
      "Sequential(\n",
      "  (conv0): ConvLayer()\n",
      "  (l_relu1): Leaky_ReLU()\n",
      "  (conv2): ConvLayer()\n",
      "  (l_relu3): Leaky_ReLU()\n",
      "  (conv4): ConvLayer()\n",
      "  (l_relu5): Leaky_ReLU()\n",
      "  (conv6): ConvLayer()\n",
      "  (l_relu7): Leaky_ReLU()\n",
      "  (pool8): MaxPool()\n",
      "  (conv9): ConvLayer()\n",
      "  (l_relu10): Leaky_ReLU()\n",
      "  (conv11): ConvLayer()\n",
      "  (l_relu12): Leaky_ReLU()\n",
      "  (pool13): MaxPool()\n",
      "  (conv14): ConvLayer()\n",
      "  (l_relu15): Leaky_ReLU()\n",
      "  (conv16): ConvLayer()\n",
      "  (l_relu17): Leaky_ReLU()\n",
      "  (pool18): MaxPool()\n",
      "  (flatten19): Flatten()\n",
      "  (dense20): Dense()\n",
      "  (l_relu21): Leaky_ReLU()\n",
      "  (drop22): Dropout()\n",
      "  (dense23): Dense()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### creating the pretraining model\n",
    "from specs import pretrain_specs\n",
    "from model import build_net\n",
    "\n",
    "print(\"Pretrain Layers:\")\n",
    "pretrain_arch = build_net(pretrain_specs)\n",
    "print(pretrain_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training the pretrained model\n",
    "from train import pretrain\n",
    "\n",
    "pretrain(os.path.join(DATA_DIR, \"data.json\"), PRETRAINED_PARAMS, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO Layers:\n",
      "Sequential(\n",
      "  (0): ConvLayer()\n",
      "  (1): Leaky_ReLU()\n",
      "  (2): ConvLayer()\n",
      "  (3): Leaky_ReLU()\n",
      "  (4): ConvLayer()\n",
      "  (5): Leaky_ReLU()\n",
      "  (6): ConvLayer()\n",
      "  (7): Leaky_ReLU()\n",
      "  (8): MaxPool()\n",
      "  (9): ConvLayer()\n",
      "  (10): Leaky_ReLU()\n",
      "  (11): ConvLayer()\n",
      "  (12): Leaky_ReLU()\n",
      "  (13): MaxPool()\n",
      "  (14): ConvLayer()\n",
      "  (15): Leaky_ReLU()\n",
      "  (16): ConvLayer()\n",
      "  (17): Leaky_ReLU()\n",
      "  (18): MaxPool()\n",
      "  (19): ConvLayer()\n",
      "  (20): Leaky_ReLU()\n",
      "  (21): ConvLayer()\n",
      "  (22): Leaky_ReLU()\n",
      "  (23): ConvLayer()\n",
      "  (24): Leaky_ReLU()\n",
      "  (25): ConvLayer()\n",
      "  (26): Leaky_ReLU()\n",
      "  (27): Flatten()\n",
      "  (28): Dense()\n",
      "  (29): Leaky_ReLU()\n",
      "  (30): Dropout()\n",
      "  (31): Dense()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Build YOLO model with pretrained parameters\n",
    "from specs import pretrain_specs, additional_yolo_specs\n",
    "from model import build_yolo_net\n",
    "\n",
    "print(\"YOLO Layers:\")\n",
    "yolo_arch = build_yolo_net(pretrain_specs, additional_yolo_specs, PRETRAINED_PARAMS)\n",
    "print(yolo_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "\n",
    "train(os.path.join(DATA_DIR, \"data.json\"), YOLO_PARAMS, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get trained YOLO model\n",
    "\n",
    "yolo_model = yolo_arch\n",
    "yolo_model.load_state_dict(torch.load(YOLO_PARAMS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Time: 0.020257949829101562\n",
      "Esimated fps: 2961.8002071368046\n",
      "Confidence: 1.7262946367263794\n",
      "Prediction: O\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=400x400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### PREDICT\n",
    "\n",
    "from boxey import id\n",
    "\n",
    "IMAGE_TO_PREDICT = f'{CATAGORIES[random.randint(0,4)]}/{random.randint(0,9)}.png'\n",
    "\n",
    "predicted = id(os.path.join(DATA_DIR, \"test\"), IMAGE_TO_PREDICT, yolo_model, CATAGORIES, jupyter=True)\n",
    "\n",
    "display(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation**\n",
    "\n",
    "For the pretraining data (an image recognition task) we used accuracy as our evaluation metric. For each element in our validation set we checked if the model predicted the right catagory and divide by the number of elements in our validation set. \n",
    "\n",
    "For the yolo proceedure evaluate, as described above is much more complicated. Due to time constraints we did not implement the mAP metric as described above; however, since this is a task in object recognition, it is possible to emperically evaluate our model. Furthermore, since we are minimizing the loss not the evaluation it doesn't effect our models ability to train. We would love to implement mAP as future work. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Conclusion**\n",
    "\n",
    "YOLO is a powerful image recognition and classification architecture, which sent waves through the deep learning community for its impressive speed and accuracy. The model has undergone several improvements, and it seems that the Vision AI company Ultralytics bought the architecture at some point; three weeks ago they released YOLO v.8.2.0, which can be found implemented in PyTorch [on their GitHub repository](https://github.com/ultralytics/ultralytics). Although many improvements have been made to the architecture and methodology, the overall structure still largely resembles YOLO v1.\n",
    "\n",
    "In this notebook, we have built a modified version of YOLO v1 which was able to train on much a less powerful machine in much less time than the full YOLO architecture. Along the way, we had to make certain sacrifices to pare down the original implementation, and doing so has been a valuable learning experience. We hope you have enjoyed this guide, and that you have found this learning as valuable as we have. The field of image recognition has been greatly changed by the introduction of neural networks and the development of YOLO, and there are sure to be many interesting developments in the field in the years to come."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs381-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc9c9e83a093b6b0dd176823cc457251ceee68774a0e93274abd6cb0b10c9ee3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
