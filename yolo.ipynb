{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementing YOLO v1 in PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <center><img src=\"./img/predictions.jpg\" width=\"750\"/> </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. What is YOLO?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <center><img src=\"./img/yolo.png\" width=\"400\"/> </center>\n",
    "</div>\n",
    "\n",
    "Developed in 2015 by Redmon et. al., YOLO (You Only Look Once) is a deep learning architecture which greatly improved the efficiency of image detection networks. The goal is simply: to detect objects appearing in an image, with their corresponding bounding boxes (see above image). Previous networks would train separately for category detection (dog, bicyle, laptop, e.g.) and bounding box detection (where an object appears in an image), going back and forth between the two to refine predictions. Redmon et. al. proposed a novel method in which bounding boxes and categories were predicted together in one pass through a single model, hence the model's name *You Only Look Once.*\n",
    "\n",
    "#### **Why is it an important problem? (SAM)**\n",
    "\n",
    "\n",
    "#### **Main contributions of YOLO (MATT)**\n",
    "\n",
    "YOLO's main contribution to the study of object recognition is its speed while maintaining correctness. As touched on above by calculating both the bounding boxes and the categorization in a single pass, YOLO completely outperformed the then current the state-of-the-art (Fast R-CNN). YOLO's original code is implemented entirely in C which also contributed to the immense speed up. Additionally compared to Fast R-CNN, YOLO made fewer background mistakes and was able to augment Fast R-CNN to produce an improved mAP score. Overall YOLO was the first object detection software that could run in real time with mostly correct predictions. Running at 45 frames per second with a corrisponding mAP score of 63.4, YOLO exceeds the mAP of other real time detectors by over 2 times; and compared to non-real-time detects only loses ~10 mAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. YOLO v1 at a High Level**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an intro\n",
    "\n",
    "#### **Data and Importing (SAM)**\n",
    "\n",
    "For our training data input, we resize each image to 112px by 112px and apply a greyscale filter. We take the pixel values in a 112x112 matrix and scale the values between -1 and 1. We take all of these scaled image matrices and feed them into our dataloader.\n",
    "<div>\n",
    "     <center><img src=\"./img/imageinput.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "#### **Architecture (MATT)**\n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/arch.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "YOLO's architecture consists of 24 convolutional layers followed by 2 fullin connected layers detailed above. The 1x1 convolutional layers are in place to reduce the feature space from preceding layers. This may be confusing at first as a 1x1 convolution with a stride of 1 just copies the image however; roughly speaking, different features of the input can be \"learned\" by different kernels of the 1x1 layer thus reducing the feature space. \n",
    "\n",
    "Object detection is a hard task, and at this point had not been achieved in real time. In order to simplify the task YOLO was first pre-trained to recognize images. For pre-training only the first 20 layers were trained followed by an average-pooling and fully connected layer --- these additional layer were truncated in the actual YOLO model.Using the ImageNet 2012 data the model was trained to recognize images from one of 20 catagories. After a week of training, the model achieved a top-5 accuracy of 88\\% (meaning each of the network contained the correct answer in the top 5 catagories it predicted 88\\% of the time).\n",
    "\n",
    "The intuitive reasoning for YOLO's pre-training can be explained with a simple example. Say I gave you an image and asked you to find every fish in the image and draw a box around it (this is essentially the YOLO task). You could probably do it --- but say you didn't know what a fish actually looked like, the task becomes significantly harder. The same is true with YOLO, by teaching it how to recognize images in a well defined dataset of ImageNet it will help make predicting bounding boxes possible.\n",
    "\n",
    "Up until the last layer, YOLO is just a standard CNN. In each layer of the CNN, it learns something about the pixel and the surrounding pixels in order to make its final prediction. To prevent overfitting a dropout layer with p = 0.5 is added after the first connected layer. Additionally YOLO randomly augments the data by changing the size, exposure and saturation to help with overfitting. To enforce non-linearity the leaky rectified linear unit (leaky ReLU) activation function is used with the parameter 0.1 following each layer besides the final layer. The final layer is special and uses a linear activation.\n",
    "\n",
    "The final prediction of YOLO is where the novelty of the model comes from. The last dense layer is reshaped into a SxSxC+(B*5) tensor where S is the chosen grid size, C is the number of catagories and B is number of boxes predicted in each grid square. YOLO uses a grid size of 7x7 and 2 boxes per square to predict across 20 catagories resulting in an output of size 7x7x30. An example is seen in the image below. This approach allows both the catagory and the bounding boxes to be predicted simultaniously for fast unified detection. Working with this output is less straightforward then something like a series of probability weights or a single value; however YOLO provides a unique loss function to handle this. \n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/grid.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "#### **Loss (SAM)**\n",
    "\n",
    "YOLO v1 implements a custom loss function, described in their paper in the following equation:\n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/lossfunction.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "This may look overwhelming at first, but it's not overly complicated. The loss function takes the SSE across the predicted $(x,y)$ box midpoint, $(h,w)$ box dimensions, and the set of $c_i$ category probabilities. Some other important symbols:\n",
    "\n",
    "$ðŸ™_{i}^{\\textrm{obj}}$ indicates whether an object appears in cell $i$\n",
    "\n",
    "$ðŸ™_{ij}^{\\textrm{obj}}$ indicates whether the $j^{\\textrm{th}}$ bounding box predictor in cell $i$ is \"responsible\" for that prediction (highest intersection over union, or IOU)\n",
    "\n",
    "$\\lambda_{\\textrm{coord}}$ is a learning parameter set to 5\n",
    "\n",
    "$\\lambda_{\\textrm{noobj}}$ is a learning parameter set to 0.5\n",
    "\n",
    "To implement this loss function, we first have to define a custom loss function in torch. We create a new `torch.nn.Module` for our custom loss, which we will call `YOLOLoss`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a `forward` function for computing the loss. The full loss function, including the extensive `forward` function, can be found in `loss.py`. We borrow most of this code from https://github.com/aladdinpersson, as while it is not a conceptually difficult loss function, the actual code implementation with torch functions becomes a bit unweildy.\n",
    "\n",
    "#### **Evaluation (MATT)**\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Our Simplified Implementation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an intro (MATT)\n",
    "\n",
    "what we did differently\n",
    "- from 20 image categories to 5 shape categories\n",
    "    - changing loss function\n",
    "- removed bounding_box_2 in loss\n",
    "- simplified network\n",
    "    - removed some layers\n",
    "    - reduced number of kernels per layer (20 vs 200)\n",
    "- overall compressing output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data and Generation (MATT)**\n",
    "\n",
    "text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from IPython.display import HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS\n",
    "\n",
    "DATA_DIR = \"DEMO\"\n",
    "TRAIN_SIZE = 100\n",
    "TEST_SIZE = 10\n",
    "PRETRAINED_PARAMS = \"models/demo_pretrain.pt\"\n",
    "YOLO_PARAMS = \"models/demo_yolo.pt\"\n",
    "CATAGORIES = ['O', 'Pent', 'Square', 'Star', 'X']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'DEMO'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# DATA GENERATION\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgenerate_data\u001b[39;00m \u001b[39mimport\u001b[39;00m generate_data\n\u001b[0;32m----> 4\u001b[0m generate_data(DATA_DIR, TRAIN_SIZE, TEST_SIZE)\n",
      "File \u001b[0;32m~/csw/cs381/Yolo-Basic/generate_data.py:170\u001b[0m, in \u001b[0;36mgenerate_data\u001b[0;34m(root, num_train, num_test)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_data\u001b[39m(root: \u001b[39mstr\u001b[39m, num_train: \u001b[39mint\u001b[39m, num_test: \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 170\u001b[0m     os\u001b[39m.\u001b[39;49mmkdir(root)\n\u001b[1;32m    171\u001b[0m     json_out \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, \u001b[39m\"\u001b[39m\u001b[39mdata.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(json_out, \u001b[39m\"\u001b[39m\u001b[39ma+\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'DEMO'"
     ]
    }
   ],
   "source": [
    "# DATA GENERATION\n",
    "from generate_data import generate_data\n",
    "\n",
    "generate_data(DATA_DIR, TRAIN_SIZE, TEST_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100%;\"><img src=\"DEMO/train/Square/84.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/O/84.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/Star/84.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/Pent/84.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/X/84.png\" style=\"width: 18%; margin: 10px; float: left;\" /></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CODE TO DISPLAY GENERATED IMAGES\n",
    "\n",
    "num = random.randint(0, TRAIN_SIZE)\n",
    "image_paths = [os.path.join(DATA_DIR, \"train\", x, str(num) + \".png\") for x in os.listdir(os.path.join(DATA_DIR, \"train\"))]\n",
    "\n",
    "images_html = ''.join(f'<img src=\"{path}\" style=\"width: 18%; margin: 10px; float: left;\" />' for path in image_paths)\n",
    "\n",
    "html_content = f'<div style=\"width: 100%;\">{images_html}</div>'\n",
    "\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/0.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.21625,0.68125,0.1925,0.1925\"\n",
      "},\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/1.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.81,0.195,0.345,0.345\"\n",
      "},\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/2.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.46375,0.18875,0.1725,0.1725\"\n",
      "},\n"
     ]
    }
   ],
   "source": [
    "### CODE TO DISPLAY GROUND TRUTH DATA ASSOCIATED WITH THE IMAGES\n",
    "\n",
    "def display_file_head(file_path, num_lines=19):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for _ in range(num_lines):\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(line.strip())\n",
    "\n",
    "# Example usage\n",
    "file_path = os.path.join(DATA_DIR, \"data.json\")\n",
    "display_file_head(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Architecture (MATT)**\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Layers:\n",
      "Sequential(\n",
      "  (conv0): ConvLayer()\n",
      "  (relu1): ReLU()\n",
      "  (conv2): ConvLayer()\n",
      "  (relu3): ReLU()\n",
      "  (conv4): ConvLayer()\n",
      "  (relu5): ReLU()\n",
      "  (conv6): ConvLayer()\n",
      "  (relu7): ReLU()\n",
      "  (pool8): MaxPool()\n",
      "  (conv9): ConvLayer()\n",
      "  (relu10): ReLU()\n",
      "  (conv11): ConvLayer()\n",
      "  (relu12): ReLU()\n",
      "  (pool13): MaxPool()\n",
      "  (conv14): ConvLayer()\n",
      "  (relu15): ReLU()\n",
      "  (conv16): ConvLayer()\n",
      "  (relu17): ReLU()\n",
      "  (pool18): MaxPool()\n",
      "  (flatten19): Flatten()\n",
      "  (dense20): Dense()\n",
      "  (relu21): ReLU()\n",
      "  (dense22): Dense()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### creating the pretraining model\n",
    "from specs import pretrain_specs\n",
    "from model import build_net\n",
    "\n",
    "print(\"Pretrain Layers:\")\n",
    "pretrain_arch = build_net(pretrain_specs)\n",
    "print(pretrain_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training the pretrained model\n",
    "from train import pretrain\n",
    "\n",
    "pretrain(os.path.join(DATA_DIR, \"data.json\"), PRETRAINED_PARAMS, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO Layers:\n",
      "Sequential(\n",
      "  (0): ConvLayer()\n",
      "  (1): ReLU()\n",
      "  (2): ConvLayer()\n",
      "  (3): ReLU()\n",
      "  (4): ConvLayer()\n",
      "  (5): ReLU()\n",
      "  (6): ConvLayer()\n",
      "  (7): ReLU()\n",
      "  (8): MaxPool()\n",
      "  (9): ConvLayer()\n",
      "  (10): ReLU()\n",
      "  (11): ConvLayer()\n",
      "  (12): ReLU()\n",
      "  (13): MaxPool()\n",
      "  (14): ConvLayer()\n",
      "  (15): ReLU()\n",
      "  (16): ConvLayer()\n",
      "  (17): ReLU()\n",
      "  (18): MaxPool()\n",
      "  (19): ConvLayer()\n",
      "  (20): ReLU()\n",
      "  (21): ConvLayer()\n",
      "  (22): ReLU()\n",
      "  (23): ConvLayer()\n",
      "  (24): ReLU()\n",
      "  (25): ConvLayer()\n",
      "  (26): ReLU()\n",
      "  (27): Flatten()\n",
      "  (28): Dense()\n",
      "  (29): ReLU()\n",
      "  (30): Dense()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Build YOLO model with pretrained parameters\n",
    "from specs import pretrain_specs, additional_yolo_specs\n",
    "from model import build_yolo_net\n",
    "\n",
    "print(\"YOLO Layers:\")\n",
    "yolo_arch = build_yolo_net(pretrain_specs, additional_yolo_specs, PRETRAINED_PARAMS)\n",
    "print(yolo_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "\n",
    "train(os.path.join(DATA_DIR, \"data.json\"), YOLO_PARAMS, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get trained YOLO model\n",
    "\n",
    "yolo_model = yolo_arch\n",
    "yolo_model.load_state_dict(torch.load(\"models/yolo.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Time: 0.01049494743347168\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=400x400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Predict\n",
    "\n",
    "from boxey import id\n",
    "\n",
    "IMAGE_TO_PREDICT = f'{CATAGORIES[random.randint(0,4)]}/{random.randint(0,9)}.png'\n",
    "\n",
    "# IMAGE_TO_PREDICT = \"O/4.png\"\n",
    "\n",
    "predicted = id(os.path.join(DATA_DIR, \"test\"), IMAGE_TO_PREDICT, yolo_model, CATAGORIES, jupyter=True)\n",
    "\n",
    "display(predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loss (SAM)**\n",
    "\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation (MATT)**\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "class YOLOLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Computes loss function according to YOLO v1.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, S = 7, B = 2, C = 20, l_coord = 5, l_noobj = 0.5):\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.l_coord = l_coord\n",
    "        self.l_noobj = l_noobj\n",
    "        self.mse = torch.nn.MSELoss(reduction=\"sum\") # SSE instead of MSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs381-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc9c9e83a093b6b0dd176823cc457251ceee68774a0e93274abd6cb0b10c9ee3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
