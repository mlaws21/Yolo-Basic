{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementing YOLO v1 in PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <center><img src=\"./img/predictions.jpg\" width=\"750\"/> </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. What is YOLO?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <center><img src=\"./img/yolo.png\" width=\"400\"/> </center>\n",
    "</div>\n",
    "\n",
    "Developed in 2015 by Redmon et. al., YOLO (You Only Look Once) is a deep learning architecture which greatly improved the efficiency of image detection networks. The goal is simply: to detect objects appearing in an image, with their corresponding bounding boxes (see above image). Previous networks would train separately for category detection (dog, bicyle, laptop, e.g.) and bounding box detection (where an object appears in an image), going back and forth between the two to refine predictions. Redmon et. al. proposed a novel method in which bounding boxes and categories were predicted together in one pass through a single model, hence the model's name *You Only Look Once.*\n",
    "\n",
    "#### **Why is it an important problem? (SAM)**\n",
    "\n",
    "\n",
    "#### **Main contributions of YOLO (MATT)**\n",
    "\n",
    "YOLO's main contribution to the study of object recognition is its speed while maintaining correctness. As touched on above by calculating both the bounding boxes and the categorization in a single pass, YOLO completely outperformed the then current the state-of-the-art (Fast R-CNN). YOLO's original code is implemented entirely in C which also contributed to the immense speed up. Additionally compared to Fast R-CNN, YOLO made fewer background mistakes and was able to augment Fast R-CNN to produce an improved mAP score. Overall YOLO was the first object detection software that could run in real time with mostly correct predictions. Running at 45 frames per second with a corrisponding mAP score of 63.4, YOLO exceeds the mAP of other real time detectors by over 2 times; and compared to non-real-time detects only loses ~10 mAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. YOLO v1 at a High Level**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an intro\n",
    "\n",
    "#### **Data and Importing (SAM)**\n",
    "\n",
    "For our training data input, we resize each image to 112px by 112px and apply a greyscale filter. We take the pixel values in a 112x112 matrix and scale the values between -1 and 1. We take all of these scaled image matrices and feed them into our dataloader.\n",
    "<div>\n",
    "     <center><img src=\"./img/imageinput.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "#### **Architecture (MATT)**\n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/arch.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "YOLO's architecture consists of 24 convolutional layers followed by 2 fullin connected layers detailed above. The 1x1 convolutional layers are in place to reduce the feature space from preceding layers. This may be confusing at first as a 1x1 convolution with a stride of 1 just copies the image however; roughly speaking, different features of the input can be \"learned\" by different kernels of the 1x1 layer thus reducing the feature space. \n",
    "\n",
    "Object detection is a hard task, and at this point had not been achieved in real time. In order to simplify the task YOLO was first pre-trained to recognize images. For pre-training only the first 20 layers were trained followed by an average-pooling and fully connected layer --- these additional layer were truncated in the actual YOLO model.Using the ImageNet 2012 data the model was trained to recognize images from one of 20 catagories. After a week of training, the model achieved a top-5 accuracy of 88\\% (meaning each of the network contained the correct answer in the top 5 catagories it predicted 88\\% of the time).\n",
    "\n",
    "The intuitive reasoning for YOLO's pre-training can be explained with a simple example. Say I gave you an image and asked you to find every fish in the image and draw a box around it (this is essentially the YOLO task). You could probably do it --- but say you didn't know what a fish actually looked like, the task becomes significantly harder. The same is true with YOLO, by teaching it how to recognize images in a well defined dataset of ImageNet it will help make predicting bounding boxes possible.\n",
    "\n",
    "Up until the last layer, YOLO is just a standard CNN. In each layer of the CNN, it learns something about the pixel and the surrounding pixels in order to make its final prediction. To prevent overfitting a dropout layer with p = 0.5 is added after the first connected layer. Additionally YOLO randomly augments the data by changing the size, exposure and saturation to help with overfitting. To enforce non-linearity the leaky rectified linear unit (leaky ReLU) activation function is used with the parameter 0.1 following each layer besides the final layer. The final layer is special and uses a linear activation.\n",
    "\n",
    "The final prediction of YOLO is where the novelty of the model comes from. The last dense layer is reshaped into a SxSxC+(B*5) tensor where S is the chosen grid size, C is the number of catagories and B is number of boxes predicted in each grid square. YOLO uses a grid size of 7x7 and 2 boxes per square to predict across 20 catagories resulting in an output of size 7x7x30. An example is seen in the image below. This approach allows both the catagory and the bounding boxes to be predicted simultaniously for fast unified detection. Working with this output is less straightforward then something like a series of probability weights or a single value; however YOLO provides a unique loss function to handle this. \n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/grid.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "#### **Loss (SAM)**\n",
    "\n",
    "YOLO v1 implements a custom loss function, described in their paper in the following equation:\n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/lossfunction.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "This may look overwhelming at first, but it's not overly complicated. The loss function takes the SSE across the predicted $(x,y)$ box midpoint, $(h,w)$ box dimensions, and the set of $c_i$ category probabilities. Some other important symbols:\n",
    "\n",
    "$ùüô_{i}^{\\textrm{obj}}$ indicates whether an object appears in cell $i$\n",
    "\n",
    "$ùüô_{ij}^{\\textrm{obj}}$ indicates whether the $j^{\\textrm{th}}$ bounding box predictor in cell $i$ is \"responsible\" for that prediction (highest intersection over union, or IOU)\n",
    "\n",
    "$\\lambda_{\\textrm{coord}}$ is a learning parameter set to 5\n",
    "\n",
    "$\\lambda_{\\textrm{noobj}}$ is a learning parameter set to 0.5\n",
    "\n",
    "To implement this loss function, we first have to define a custom loss function in torch. We create a new `torch.nn.Module` for our custom loss, which we will call `YOLOLoss`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a `forward` function for computing the loss. The full loss function, including the extensive `forward` function, can be found in `loss.py`. We borrow most of this code from https://github.com/aladdinpersson, as while it is not a conceptually difficult loss function, the actual code implementation with torch functions becomes a bit unweildy.\n",
    "\n",
    "#### **Evaluation (MATT)**\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Our Simplified Implementation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a very basic version of YOLO called *YOLO-Basic* that can perform a simpler task than the original YOLO due to computing and time constraints. \n",
    "\n",
    "In order to speed up the YOLO training pipeline we made three significant changes. Firstly, we used synthetically genereated images of 5 different shapes (X, Circle, Square, Rectangle, Star. This helped remove background noise as shapes are generated on a plain white background allowing for easier recognition. It also reduced the dimensionality of the output because there were less catagories to predict. Secondly we reduce the size of the final output grid from 7x7 to 3x3 to greatly reduce the amount of predictions the model makes each batch. To that end we also have each grid cell only predict a single bounding box. These two changes combined greatly simplify our output as we have gone from a 7x7x(20 + 2\\*5) = 1470 parameter output shape to a 3x3x(5 + 1\\*5) = 90 parameter output shape. This reduction of dimentionality allows for out last significant change which was simplifying the network. We removed several convolutional layers and greatly reduced the kernel size at each layer to 32, down from some kernel sizes over 1000. Lastly, we provide the images as 112x112 images to simplify the input; without background noise sharper images are not needed for identification.\n",
    "\n",
    "Below we explain the simplification we did in more detail and provide a easily runnable set of code cells to run *YOLO-Basic* from start to finish. These code cells mostly import functions from other python files that we wrote so if you are concerned with implementation details you should view the other .py files in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS \n",
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from IPython.display import HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS\n",
    "\n",
    "DATA_DIR = \"DEMO\"\n",
    "TRAIN_SIZE = 100\n",
    "TEST_SIZE = 10\n",
    "PRETRAINED_PARAMS = \"models/demo_pretrain.pt\"\n",
    "YOLO_PARAMS = \"models/demo_yolo.pt\"\n",
    "CATAGORIES = ['O', 'Pent', 'Square', 'Star', 'X']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data and Generation (MATT)**\n",
    "\n",
    "As noted in the introduction we used synthetically generated data to train and test our *YOLO-Basic* model. To generate the images we used the python PIL library. We wrote 5 different functions that each generated a shape of random size in a random position on the screen. Each shape had an associated bounding box that completely surrounded the shape plus a small adjustment factor. Shapes were generated to be of reasonable size (between 1/8 and 1/2 of the image) and such that the entire shape was contained in the frame of the image. Example images can be seen after generating the data by running the corrisponding cell. \n",
    "\n",
    "To simulate the training proceedure of YOLO, the pretraining set is a different set of images than the YOLO training set. The images are generated as detailed above with 1000 images of each shape for training and 100 images of each shape for validation. These images were no longer used after the pretraining was completed.\n",
    "\n",
    "To generate the YOLO training and testing data, 10,000 images of each shape were generated to use as a training set for a total of 50,000 training images; 100 images of each shape were generated as a validation set to monitor the accuracy. With each image that was created an entry was also created into the data.json file for a given data directory. Eeach entry contains the shape label, the path to the image file, the partition (train / test) and the bounding box. The data loader can then use this json file to read in the data for training and testing. A preview of this json can be seen by running the corrisponding cell below.\n",
    "\n",
    "Note: while multiple shape generation is possible given our functions we only generated images with a single shape for training and testing for simplicity and time sake; however in theory *YOLO-Basic* could still recognize mulitple shapes in a single image.  \n",
    "\n",
    "To generate and visualize the data run the cells below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA GENERATION\n",
    "from generate_data import generate_data\n",
    "\n",
    "generate_data(DATA_DIR, TRAIN_SIZE, TEST_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100%;\"><img src=\"DEMO/train/Square/41.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/O/41.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/Star/41.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/Pent/41.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/X/41.png\" style=\"width: 18%; margin: 10px; float: left;\" /></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DISPLAY GENERATED IMAGES\n",
    "\n",
    "num = random.randint(0, TRAIN_SIZE)\n",
    "image_paths = [os.path.join(DATA_DIR, \"train\", x, str(num) + \".png\") for x in os.listdir(os.path.join(DATA_DIR, \"train\"))]\n",
    "\n",
    "images_html = ''.join(f'<img src=\"{path}\" style=\"width: 18%; margin: 10px; float: left;\" />' for path in image_paths)\n",
    "\n",
    "html_content = f'<div style=\"width: 100%;\">{images_html}</div>'\n",
    "\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/0.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.58875,0.80875,0.1025,0.1025\"\n",
      "},\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/1.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.78875,0.34625,0.2775,0.2775\"\n",
      "},\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/2.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.7325,0.63,0.105,0.105\"\n",
      "},\n"
     ]
    }
   ],
   "source": [
    "### DISPLAY JSON DATA\n",
    "\n",
    "# json data stores ground truth data associated with the images.\n",
    "def display_file_head(file_path, num_lines=19):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for _ in range(num_lines):\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(line.strip())\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, \"data.json\")\n",
    "display_file_head(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Architecture (MATT)**\n",
    "\n",
    "- pretrain 20 -> 8\n",
    "- yolo 24 -> 12\n",
    "- additional maxpool at the end\n",
    "- all kernel size 32\n",
    "- model format\n",
    "\n",
    "Overall, as mentioned in the intro, to fit our computational and time constraints, we greatly reduced the complexity of the models. We reduced the number of pretraining layers from 20 to 8 \n",
    "\n",
    "\n",
    "Additionally we used ReLU instead of Leaky ReLU for activation, this seemed to train a little more consistantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Layers:\n",
      "Sequential(\n",
      "  (conv0): ConvLayer()\n",
      "  (relu1): ReLU()\n",
      "  (conv2): ConvLayer()\n",
      "  (relu3): ReLU()\n",
      "  (conv4): ConvLayer()\n",
      "  (relu5): ReLU()\n",
      "  (conv6): ConvLayer()\n",
      "  (relu7): ReLU()\n",
      "  (pool8): MaxPool()\n",
      "  (conv9): ConvLayer()\n",
      "  (relu10): ReLU()\n",
      "  (conv11): ConvLayer()\n",
      "  (relu12): ReLU()\n",
      "  (pool13): MaxPool()\n",
      "  (conv14): ConvLayer()\n",
      "  (relu15): ReLU()\n",
      "  (conv16): ConvLayer()\n",
      "  (relu17): ReLU()\n",
      "  (pool18): MaxPool()\n",
      "  (flatten19): Flatten()\n",
      "  (dense20): Dense()\n",
      "  (relu21): ReLU()\n",
      "  (dense22): Dense()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### creating the pretraining model\n",
    "from specs import pretrain_specs\n",
    "from model import build_net\n",
    "\n",
    "print(\"Pretrain Layers:\")\n",
    "pretrain_arch = build_net(pretrain_specs)\n",
    "print(pretrain_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training the pretrained model\n",
    "from train import pretrain\n",
    "\n",
    "pretrain(os.path.join(DATA_DIR, \"data.json\"), PRETRAINED_PARAMS, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO Layers:\n",
      "Sequential(\n",
      "  (0): ConvLayer()\n",
      "  (1): ReLU()\n",
      "  (2): ConvLayer()\n",
      "  (3): ReLU()\n",
      "  (4): ConvLayer()\n",
      "  (5): ReLU()\n",
      "  (6): ConvLayer()\n",
      "  (7): ReLU()\n",
      "  (8): MaxPool()\n",
      "  (9): ConvLayer()\n",
      "  (10): ReLU()\n",
      "  (11): ConvLayer()\n",
      "  (12): ReLU()\n",
      "  (13): MaxPool()\n",
      "  (14): ConvLayer()\n",
      "  (15): ReLU()\n",
      "  (16): ConvLayer()\n",
      "  (17): ReLU()\n",
      "  (18): MaxPool()\n",
      "  (19): ConvLayer()\n",
      "  (20): ReLU()\n",
      "  (21): ConvLayer()\n",
      "  (22): ReLU()\n",
      "  (23): ConvLayer()\n",
      "  (24): ReLU()\n",
      "  (25): ConvLayer()\n",
      "  (26): ReLU()\n",
      "  (27): Flatten()\n",
      "  (28): Dense()\n",
      "  (29): ReLU()\n",
      "  (30): Dense()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### Build YOLO model with pretrained parameters\n",
    "from specs import pretrain_specs, additional_yolo_specs\n",
    "from model import build_yolo_net\n",
    "\n",
    "print(\"YOLO Layers:\")\n",
    "yolo_arch = build_yolo_net(pretrain_specs, additional_yolo_specs, PRETRAINED_PARAMS)\n",
    "print(yolo_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "\n",
    "train(os.path.join(DATA_DIR, \"data.json\"), YOLO_PARAMS, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get trained YOLO model\n",
    "\n",
    "yolo_model = yolo_arch\n",
    "yolo_model.load_state_dict(torch.load(\"models/yolo.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Time: 0.019006967544555664\n",
      "Prediction: Star\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=400x400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### PREDICT\n",
    "\n",
    "from boxey import id\n",
    "\n",
    "IMAGE_TO_PREDICT = f'{CATAGORIES[random.randint(0,4)]}/{random.randint(0,9)}.png'\n",
    "\n",
    "# IMAGE_TO_PREDICT = \"O/6.png\"\n",
    "\n",
    "predicted = id(os.path.join(DATA_DIR, \"test\"), IMAGE_TO_PREDICT, yolo_model, CATAGORIES, jupyter=True)\n",
    "\n",
    "display(predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loss (SAM)**\n",
    "\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation (MATT)**\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "class YOLOLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Computes loss function according to YOLO v1.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, S = 7, B = 2, C = 20, l_coord = 5, l_noobj = 0.5):\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.l_coord = l_coord\n",
    "        self.l_noobj = l_noobj\n",
    "        self.mse = torch.nn.MSELoss(reduction=\"sum\") # SSE instead of MSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs381-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc9c9e83a093b6b0dd176823cc457251ceee68774a0e93274abd6cb0b10c9ee3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
