{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementing YOLO v1 in PyTorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <center><img src=\"./img/predictions.jpg\" width=\"750\"/> </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. What is YOLO?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <center><img src=\"./img/yolo.png\" width=\"400\"/> </center>\n",
    "</div>\n",
    "\n",
    "Developed in 2015 by Redmon et. al., YOLO (You Only Look Once) is a deep learning architecture which greatly improved the efficiency of image detection networks. The goal is simply: to detect objects appearing in an image, with their corresponding bounding boxes (see above image). Previous networks would train separately for category detection (dog, bicyle, laptop, e.g.) and bounding box detection (where an object appears in an image), going back and forth between the two to refine predictions. Redmon et. al. proposed a novel method in which bounding boxes and categories were predicted together in one pass through a single model, hence the model's name *You Only Look Once.*\n",
    "\n",
    "#### **Why is it an important problem? (SAM)**\n",
    "\n",
    "\n",
    "#### **Main contributions of YOLO (MATT)**\n",
    "\n",
    "YOLO's main contribution to the study of object recognition is its speed while maintaining correctness. As touched on above by calculating both the bounding boxes and the categorization in a single pass, YOLO completely outperformed the then current the state-of-the-art (Fast R-CNN). YOLO's original code is implemented entirely in C which also contributed to the immense speed up. Additionally compared to Fast R-CNN, YOLO made fewer background mistakes and was able to augment Fast R-CNN to produce an improved mAP score. Overall YOLO was the first object detection software that could run in real time with mostly correct predictions. Running at 45 frames per second with a corrisponding mAP score of 63.4, YOLO exceeds the mAP of other real time detectors by over 2 times; and compared to non-real-time detects only loses ~10 mAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. YOLO v1 at a High Level**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an intro\n",
    "\n",
    "#### **Data and Importing (SAM)**\n",
    "\n",
    "TODO FIX THIS: THIS should talk about the data that YOLO paper uses\n",
    "For our training data input, we resize each image to 112px by 112px and apply a greyscale filter. We take the pixel values in a 112x112 matrix and scale the values between -1 and 1. We take all of these scaled image matrices and feed them into our dataloader.\n",
    "<div>\n",
    "     <center><img src=\"./img/imageinput.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "#### **Architecture (MATT)**\n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/arch.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "YOLO's architecture consists of 24 convolutional layers followed by 2 fullin connected layers detailed above. The 1x1 convolutional layers are in place to reduce the feature space from preceding layers. This may be confusing at first as a 1x1 convolution with a stride of 1 just copies the image however; roughly speaking, different features of the input can be \"learned\" by different kernels of the 1x1 layer thus reducing the feature space. \n",
    "\n",
    "Object detection is a hard task, and at this point had not been achieved in real time. In order to simplify the task YOLO was first pre-trained to recognize images. For pre-training only the first 20 layers were trained followed by an average-pooling and fully connected layer --- these additional layer were truncated in the actual YOLO model.Using the ImageNet 2012 data the model was trained to recognize images from one of 20 catagories. After a week of training, the model achieved a top-5 accuracy of 88\\% (meaning each of the network contained the correct answer in the top 5 catagories it predicted 88\\% of the time).\n",
    "\n",
    "The intuitive reasoning for YOLO's pre-training can be explained with a simple example. Say I gave you an image and asked you to find every fish in the image and draw a box around it (this is essentially the YOLO task). You could probably do it --- but say you didn't know what a fish actually looked like, the task becomes significantly harder. The same is true with YOLO, by teaching it how to recognize images in a well defined dataset of ImageNet it will help make predicting bounding boxes possible.\n",
    "\n",
    "Up until the last layer, YOLO is just a standard CNN. In each layer of the CNN, it learns something about the pixel and the surrounding pixels in order to make its final prediction. To prevent overfitting a dropout layer with p = 0.5 is added after the first connected layer. Additionally YOLO randomly augments the data by changing the size, exposure and saturation to help with overfitting. To enforce non-linearity the leaky rectified linear unit (leaky ReLU) activation function is used with the parameter 0.1 following each layer besides the final layer. The final layer is special and uses a linear activation.\n",
    "\n",
    "The final prediction of YOLO is where the novelty of the model comes from. The last dense layer is reshaped into a SxSxC+(B*5) tensor where S is the chosen grid size, C is the number of catagories and B is number of boxes predicted in each grid square. YOLO uses a grid size of 7x7 and 2 boxes per square to predict across 20 catagories resulting in an output of size 7x7x30. An example is seen in the image below. This approach allows both the catagory and the bounding boxes to be predicted simultaniously for fast unified detection. Working with this output is less straightforward then something like a series of probability weights or a single value; however YOLO provides a unique loss function to handle this. \n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/grid.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "#### **Loss (SAM)**\n",
    "\n",
    "YOLO v1 implements a custom loss function, described in their paper in the following equation:\n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/lossfunction.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    "\n",
    "This may look overwhelming at first, but it's not overly complicated. The loss function takes the SSE across the predicted $(x,y)$ box midpoint, $(h,w)$ box dimensions, and the set of $c_i$ category probabilities. Some other important symbols:\n",
    "\n",
    "$ùüô_{i}^{\\textrm{obj}}$ indicates whether an object appears in cell $i$\n",
    "\n",
    "$ùüô_{ij}^{\\textrm{obj}}$ indicates whether the $j^{\\textrm{th}}$ bounding box predictor in cell $i$ is \"responsible\" for that prediction (highest intersection over union, or IOU)\n",
    "\n",
    "$\\lambda_{\\textrm{coord}}$ is a learning parameter set to 5\n",
    "\n",
    "$\\lambda_{\\textrm{noobj}}$ is a learning parameter set to 0.5\n",
    "\n",
    "\n",
    "[TODO this should be below, also we dont define it as a layer anymore just a function]\n",
    "[TODO mention IOU]\n",
    "To implement this loss function, we first have to define a custom loss function in torch. We create a new `torch.nn.Module` for our custom loss, which we will call `YOLOLoss`:\n",
    "\n",
    "We then define a `forward` function for computing the loss. The full loss function, including the extensive `forward` function, can be found in `loss.py`. We borrow most of this code from https://github.com/aladdinpersson, as while it is not a conceptually difficult loss function, the actual code implementation with torch functions becomes a bit unweildy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Evaluation (MATT)**\n",
    "\n",
    "There are two main metrics that YOLO evaluates itself on: speed and mean average precision (mAP). Since YOLO is a real time detection tool its speed in measured by how many image frames can it predict per second (fps). This metric is important because this is the fps it can accurately translate video at. YOLO is able to translate at 45fps, significantly faster than any detector with similar mAP scores. For reference the human eye on average can only see between 30fps and 60fps, thus 45fps is plenty fast for real time translation. Fast YOLO, a smaller version translates at 155fps for a moderate hit to mAP.\n",
    "\n",
    "The second evaluation metric is mAP. Essentially, mAP is a measure of correctness of the prediction. Generally speaking mAP refers to the area under the precision-recall curve. To understand this first we need to understand precision and recall. Precision measures of all positive predicts the model made how many were correct, i.e. $\\frac{\\text{correct positive predictions}}{\\text{all positive predictions}}$ where in our case a \"positive prediction\" refers to predicting an object. Recall measures of all predictions how many were positive and correct i.e. $\\frac{\\text{correct positive predictions}}{\\text{all predictions}}$. Each of these metrics on there own are not particularly useful as predicting positive for all instances gets a perfect recall and predicting a small subset of easy to predict images gets perfect precision. However, if we can encapsulate both metrics into one then we can optimize it. We do this using the precison-recall curve which plots precison as a function of recall. To evaluate then all we have to do is take the integral of the precison-recall curve. \n",
    "\n",
    "For object detection the input to the recall and loss functions is whether or not the box is correct. To determine that we measure the Intersection Over Union (IOU) between the predicted boxes and the ground truth boxes in the image. If the IOU is over some threshold we declare that the box is correct otherwise we declare that it is incorrect. The threshold in this case is dependent on how \"good\" we want the boxes to be. The YOLO paper doesn't say what threshold they use, but other implementations use 80\\%.\n",
    "\n",
    "For its speed YOLO significantly outperforms the state-of-the-art mAP scores only losing to Fast R-CNN, and Faster R-CNN VGG-16 that run at 0.5 and 7fps respectively. \n",
    "\n",
    "Below we provide a table that compares the mAP and fps of different object detectors.  \n",
    "\n",
    "<div>\n",
    "     <center><img src=\"./img/eval.png\" width=\"750\"/> </center>\n",
    "</div>\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Our Simplified Implementation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a very basic version of YOLO called *YOLO-Basic* that can perform a simpler task than the original YOLO due to computing and time constraints. \n",
    "\n",
    "In order to speed up the YOLO training pipeline we made three significant changes. Firstly, we used synthetically genereated images of 5 different shapes (X, Circle, Square, Rectangle, Star. This helped remove background noise as shapes are generated on a plain white background allowing for easier recognition. It also reduced the dimensionality of the output because there were less catagories to predict. Secondly we reduce the size of the final output grid from 7x7 to 3x3 to greatly reduce the amount of predictions the model makes each minibatch. To that end we also have each grid cell only predict a single bounding box. These two changes combined greatly simplify our output as we have gone from a 7x7x(20 + 2\\*5) = 1470 parameter output shape to a 3x3x(5 + 1\\*5) = 90 parameter output shape. This reduction of dimentionality allows for out last significant change which was simplifying the network. We removed several convolutional layers and greatly reduced the kernel size at each layer to 32, down from some kernel sizes over 1000. Lastly, we provide the images as 112x112 images to simplify the input; without background noise sharper images are not needed for identification.\n",
    "\n",
    "Below we explain the simplification we did in more detail and provide a easily runnable set of code cells to run *YOLO-Basic* from start to finish. These code cells mostly import functions from other python files that we wrote so if you are concerned with implementation details you should view the other .py files in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS \n",
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from IPython.display import HTML, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS\n",
    "\n",
    "DATA_DIR = \"DEMO\"\n",
    "TRAIN_SIZE = 100\n",
    "TEST_SIZE = 10\n",
    "PRETRAINED_PARAMS = \"models/demo_pretrain.pt\"\n",
    "YOLO_PARAMS = \"models/demo_yolo.pt\"\n",
    "CATAGORIES = ['O', 'Pent', 'Square', 'Star', 'X']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data and Generation (MATT)**\n",
    "\n",
    "As noted in the introduction we used synthetically generated data to train and test our *YOLO-Basic* model. To generate the images we used the python PIL library. We wrote 5 different functions that each generated a shape of random size in a random position on the screen. Each shape had an associated bounding box that completely surrounded the shape plus a small adjustment factor. Shapes were generated to be of reasonable size (between 1/8 and 1/2 of the image) and such that the entire shape was contained in the frame of the image. Example images can be seen after generating the data by running the corrisponding cell. \n",
    "\n",
    "To simulate the training proceedure of YOLO, the pretraining set is a different set of images than the YOLO training set. The images are generated as detailed above with 1000 images of each shape for training and 100 images of each shape for validation. These images were no longer used after the pretraining was completed.\n",
    "\n",
    "To generate the YOLO training and testing data, 10,000 images of each shape were generated to use as a training set for a total of 50,000 training images; 100 images of each shape were generated as a validation set to monitor the accuracy. With each image that was created an entry was also created into the data.json file for a given data directory. Eeach entry contains the shape label, the path to the image file, the partition (train / test) and the bounding box. The data loader can then use this json file to read in the data for training and testing. A preview of this json can be seen by running the corrisponding cell below.\n",
    "\n",
    "Note: while multiple shape generation is possible given our functions we only generated images with a single shape for training and testing for simplicity and time sake; however in theory *YOLO-Basic* could still recognize mulitple shapes in a single image.  \n",
    "\n",
    "To generate and visualize the data run the cells below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Data Directory DEMO already exists\n",
      "if you would like to continue run rm -r DEMO in your terminal\n"
     ]
    }
   ],
   "source": [
    "# DATA GENERATION\n",
    "from generate_data import generate_data\n",
    "\n",
    "generate_data(DATA_DIR, TRAIN_SIZE, TEST_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100%;\"><img src=\"DEMO/train/Square/71.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/O/71.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/Star/71.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/Pent/71.png\" style=\"width: 18%; margin: 10px; float: left;\" /><img src=\"DEMO/train/X/71.png\" style=\"width: 18%; margin: 10px; float: left;\" /></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DISPLAY GENERATED IMAGES\n",
    "\n",
    "num = random.randint(0, TRAIN_SIZE)\n",
    "image_paths = [os.path.join(DATA_DIR, \"train\", x, str(num) + \".png\") for x in os.listdir(os.path.join(DATA_DIR, \"train\"))]\n",
    "\n",
    "images_html = ''.join(f'<img src=\"{path}\" style=\"width: 18%; margin: 10px; float: left;\" />' for path in image_paths)\n",
    "\n",
    "html_content = f'<div style=\"width: 100%;\">{images_html}</div>'\n",
    "\n",
    "HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/0.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.58875,0.80875,0.1025,0.1025\"\n",
      "},\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/1.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.78875,0.34625,0.2775,0.2775\"\n",
      "},\n",
      "{\n",
      "\"label\": \"X\",\n",
      "\"filename\": \"DEMO/train/X/2.png\",\n",
      "\"partition\": \"train\",\n",
      "\"box\": \"1,0.7325,0.63,0.105,0.105\"\n",
      "},\n"
     ]
    }
   ],
   "source": [
    "### DISPLAY JSON DATA\n",
    "\n",
    "# json data stores ground truth data associated with the images.\n",
    "def display_file_head(file_path, num_lines=19):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for _ in range(num_lines):\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(line.strip())\n",
    "\n",
    "file_path = os.path.join(DATA_DIR, \"data.json\")\n",
    "display_file_head(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Architecture and Training(MATT)**\n",
    "\n",
    "Overall, as mentioned in the intro, to fit our computational and time constraints, we greatly reduced the complexity of the models. We reduced the number of pretraining convolutional layers from 20 to 8 and the number of kernel all down to 32. We use the same amount of max pooling layer and layers with stride > 1 in order to reduce to the same grid size; however we add one extra maxpool layer at the end of the pretrain model to reduce what will become the grid size in yolo training to 3x3. \n",
    "\n",
    "For yolo training we add 4 new untrained layers as the paper suggests; however our total is sill 12 layers vs 24 layers. Furthermore, as in pretraining the number of kernels are all 32 down from 1024. \n",
    "\n",
    "For both pretraining and yolo training we use Leaky ReLU as our activation function with parameter 0.1 as described in the paper. We also implement a dropout layer with p = 0.5 between our fully connected layers as described by the paper. \n",
    "\n",
    "For training we used minibatch gradient decent with a batch size of 32 for both pretraining and yolo training. Pretraining was able to quickly converge to perfect accuracy on both the training and test set and stopped training early around 5 epochs. YOLO training ran for [TODO] epochs before it converged. We used the ADAM optimizer for our decent with learning rate of 0.001 and the default ADAM parameters.\n",
    "\n",
    "To build and train the model run the cells below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain Layers:\n",
      "Sequential(\n",
      "  (conv0): ConvLayer()\n",
      "  (l_relu1): Leaky_ReLU()\n",
      "  (conv2): ConvLayer()\n",
      "  (l_relu3): Leaky_ReLU()\n",
      "  (conv4): ConvLayer()\n",
      "  (l_relu5): Leaky_ReLU()\n",
      "  (conv6): ConvLayer()\n",
      "  (l_relu7): Leaky_ReLU()\n",
      "  (pool8): MaxPool()\n",
      "  (conv9): ConvLayer()\n",
      "  (l_relu10): Leaky_ReLU()\n",
      "  (conv11): ConvLayer()\n",
      "  (l_relu12): Leaky_ReLU()\n",
      "  (pool13): MaxPool()\n",
      "  (conv14): ConvLayer()\n",
      "  (l_relu15): Leaky_ReLU()\n",
      "  (conv16): ConvLayer()\n",
      "  (l_relu17): Leaky_ReLU()\n",
      "  (pool18): MaxPool()\n",
      "  (flatten19): Flatten()\n",
      "  (dense20): Dense()\n",
      "  (l_relu21): Leaky_ReLU()\n",
      "  (drop22): Dropout()\n",
      "  (dense23): Dense()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### creating the pretraining model\n",
    "from specs import pretrain_specs\n",
    "from model import build_net\n",
    "\n",
    "print(\"Pretrain Layers:\")\n",
    "pretrain_arch = build_net(pretrain_specs)\n",
    "print(pretrain_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training the pretrained model\n",
    "from train import pretrain\n",
    "\n",
    "pretrain(os.path.join(DATA_DIR, \"data.json\"), PRETRAINED_PARAMS, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO Layers:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"dense23.weight\". \n\tUnexpected key(s) in state_dict: \"dense22.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m build_yolo_net\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mYOLO Layers:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m yolo_arch \u001b[39m=\u001b[39m build_yolo_net(pretrain_specs, additional_yolo_specs, PRETRAINED_PARAMS)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(yolo_arch)\n",
      "File \u001b[0;32m~/csw/cs381/Yolo-Basic/model.py:154\u001b[0m, in \u001b[0;36mbuild_yolo_net\u001b[0;34m(pt_specs, yolo_added_spec, pt_file)\u001b[0m\n\u001b[1;32m    152\u001b[0m model \u001b[39m=\u001b[39m build_net(pt_specs)\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m pt_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(pt_file))\n\u001b[1;32m    156\u001b[0m net \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m    157\u001b[0m ctr \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/csw/cs381/cs381-env/lib/python3.11/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"dense23.weight\". \n\tUnexpected key(s) in state_dict: \"dense22.weight\". "
     ]
    }
   ],
   "source": [
    "### Build YOLO model with pretrained parameters\n",
    "from specs import pretrain_specs, additional_yolo_specs\n",
    "from model import build_yolo_net\n",
    "\n",
    "print(\"YOLO Layers:\")\n",
    "yolo_arch = build_yolo_net(pretrain_specs, additional_yolo_specs, PRETRAINED_PARAMS)\n",
    "print(yolo_arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "\n",
    "train(os.path.join(DATA_DIR, \"data.json\"), YOLO_PARAMS, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get trained YOLO model\n",
    "\n",
    "yolo_model = yolo_arch\n",
    "yolo_model.load_state_dict(torch.load(\"models/yolo.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Time: 0.010735750198364258\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=400x400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### PREDICT\n",
    "\n",
    "from boxey import id\n",
    "\n",
    "IMAGE_TO_PREDICT = f'{CATAGORIES[random.randint(0,4)]}/{random.randint(0,9)}.png'\n",
    "\n",
    "# IMAGE_TO_PREDICT = \"O/6.png\"\n",
    "\n",
    "predicted = id(os.path.join(DATA_DIR, \"test\"), IMAGE_TO_PREDICT, yolo_model, CATAGORIES, jupyter=True)\n",
    "\n",
    "display(predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Loss (SAM)**\n",
    "\n",
    "text\n",
    "talk about pretrian loss too\n",
    "\n",
    "maybe move up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation (MATT)**\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs381-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc9c9e83a093b6b0dd176823cc457251ceee68774a0e93274abd6cb0b10c9ee3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
